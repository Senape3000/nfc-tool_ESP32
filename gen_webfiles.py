#!/usr/bin/env python3
"""
Generate webFiles.h from HTML/CSS/JS files in data/web_server/
Minifies and compresses with GZIP, outputs as C++ header with PROGMEM arrays.

Usage:
    python gen_webfiles.py
    
This will:
    1. Read from data/web_server/*.{html,css,js}
    2. Minify each file
    3. Compress with GZIP
    4. Generate include/webFiles.h with C++ arrays
"""

import os
import glob
import gzip
import hashlib
from pathlib import Path

# Directories
PROJECT_DIR = Path(__file__).parent.parent
DATA_DIR = PROJECT_DIR / "nfc-tool_ESP32" / "web_server"
INCLUDE_DIR = PROJECT_DIR / "nfc-tool_ESP32" / "include"
HEADER_FILE = INCLUDE_DIR / "webFiles.h"
CHECKSUM_FILE = DATA_DIR / "checksum.sha256"

# File types to process
FILE_TYPES = ["html", "css", "js"]


def hash_files(file_paths):
    """Generate SHA256 hash for multiple files."""
    combined_hash = hashlib.sha256()
    for file_path in sorted(file_paths):
        with open(file_path, "rb") as f:
            file_hash = hashlib.sha256(f.read()).hexdigest()
            combined_hash.update(file_hash.encode("utf-8"))
    return combined_hash.hexdigest()


def load_checksum():
    """Load previous checksum if it exists."""
    if CHECKSUM_FILE.exists():
        with open(CHECKSUM_FILE, "r") as f:
            return f.read().strip()
    return None


def save_checksum(checksum):
    """Save current checksum."""
    with open(CHECKSUM_FILE, "w") as f:
        f.write(checksum)


def minify_html(content):
    """Basic HTML minification."""
    # Remove comments
    while "<!--" in content:
        start = content.find("<!--")
        end = content.find("-->", start)
        if end != -1:
            content = content[:start] + content[end + 3 :]
    
    # Remove excess whitespace
    content = " ".join(content.split())
    return content


def minify_css(content):
    """Basic CSS minification."""
    # Remove comments
    while "/*" in content:
        start = content.find("/*")
        end = content.find("*/", start)
        if end != -1:
            content = content[:start] + content[end + 2 :]
    
    # Remove excess whitespace
    content = content.replace("\n", "").replace("\r", "")
    content = " ".join(content.split())
    
    # Remove spaces around certain characters
    for char in "{}:;,>+~[]":
        content = content.replace(f" {char}", char)
        content = content.replace(f"{char} ", char)
    
    return content


def minify_js(content):
    """Basic JavaScript minification."""
    # Remove comments
    lines = []
    for line in content.split("\n"):
        line = line.split("//")[0]  # Remove line comments
        lines.append(line)
    content = "\n".join(lines)
    
    # Remove /* */ comments
    while "/*" in content:
        start = content.find("/*")
        end = content.find("*/", start)
        if end != -1:
            content = content[:start] + content[end + 2 :]
    
    # Remove excess whitespace
    content = " ".join(content.split())
    return content


def minify(file_path, file_type):
    """Minify based on file type."""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    if file_type == "html":
        return minify_html(content)
    elif file_type == "css":
        return minify_css(content)
    elif file_type == "js":
        return minify_js(content)
    
    return content


def process_files():
    """Main processing function."""
    
    # Check if source directory exists
    if not DATA_DIR.exists():
        print(f"❌ Error: Source directory '{DATA_DIR}' does not exist!")
        return False
    
    # Find all files to process
    files_to_process = []
    for ext in FILE_TYPES:
        pattern = DATA_DIR / f"*.{ext}"
        files_to_process.extend(glob.glob(str(pattern)))
    
    if not files_to_process:
        print(f"⚠️  Warning: No {FILE_TYPES} files found in '{DATA_DIR}'")
        return False
    
    # Check if we need to regenerate (using checksum)
    current_checksum = hash_files(files_to_process)
    previous_checksum = load_checksum()
    
    if current_checksum == previous_checksum:
        print("✓ [SKIP] Files unchanged, webFiles.h is up-to-date")
        return True
    
    print(f"✓ [GENERATE] Processing {len(files_to_process)} file(s)...")
    
    # Create include directory
    INCLUDE_DIR.mkdir(parents=True, exist_ok=True)
    
    # Generate header file
    with open(HEADER_FILE, "w", encoding="utf-8") as header:
        # Write header guards and includes
        header.write("#ifndef WEB_FILES_H\n")
        header.write("#define WEB_FILES_H\n\n")
        header.write("#include <stdint.h>\n\n")
        header.write("// THIS FILE IS AUTOGENERATED - DO NOT MODIFY\n")
        header.write("// Regenerate with: python gen_webfiles.py\n\n")
        
        # Process each file
        total_original = 0
        total_compressed = 0
        
        for file_path in sorted(files_to_process):
            file_path = Path(file_path)
            file_type = file_path.suffix[1:].lower()
            
            # Read and minify
            minified = minify(file_path, file_type)
            minified_bytes = minified.encode("utf-8")
            
            # Compress with GZIP
            compressed = gzip.compress(minified_bytes)
            
            # Get variable name
            var_name = file_path.stem.replace("-", "_") + "_web"
            
            # Calculate sizes
            original_size = len(minified_bytes)
            compressed_size = len(compressed)
            compression_ratio = (1 - compressed_size / original_size) * 100
            
            total_original += original_size
            total_compressed += compressed_size
            
            print(f"  {file_path.name:20} | {original_size:6} → {compressed_size:6} bytes ({compression_ratio:5.1f}%)")
            
            # Write to header
            header.write(f"// {file_path.name}\n")
            header.write(f"const uint8_t {var_name}[] PROGMEM = {{\n")
            
            # Write hex values (15 bytes per line)
            for i in range(0, len(compressed), 15):
                chunk = compressed[i : i + 15]
                hex_values = ", ".join(f"0x{byte:02X}" for byte in chunk)
                header.write(f"    {hex_values},\n")
            
            header.write(f"}};\n")
            header.write(f"const uint32_t {var_name}_size = {compressed_size};\n\n")
        
        # Write closing
        header.write("#endif // WEB_FILES_H\n")
    
    # Save checksum
    save_checksum(current_checksum)
    
    # Print summary
    if total_original > 0:
        total_ratio = (1 - total_compressed / total_original) * 100
        print(f"\n✓ [DONE] Total: {total_original} → {total_compressed} bytes ({total_ratio:.1f}%)")
    
    print(f"✓ Generated: {HEADER_FILE}")
    return True


if __name__ == "__main__":
    success = process_files()
    exit(0 if success else 1)
